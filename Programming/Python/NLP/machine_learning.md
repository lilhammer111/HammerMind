# Basic Terms

**Feature:** 特征，自变量



**Model:**算法模型，即函数，模型可以是univariate linear regression线性回归模型，也可以是Deep Learning，多层感知机这些模型。



**Predication:** 输出，因变量



**Linear combinations(or linear score functions):**
$$
f_{\vec w,b}(\vec x) = \vec w \cdot {\vec x} + b
$$
**Loss Function:**损失函数，通常指衡量单一样本预测与实际值之间差异的函数。例如，在一个数据点上的平方误差或二元交叉熵。例如：
$$
L(\hat y,y) = -[ y\ \log⁡(\hat y)+(1−y)\ \log⁡(1−\hat y)]
$$


**Cost Function:**通常是整个训练集上所有样本损失的平均值或总和，有时也称为“目标函数”或“优化目标”。成本函数是损失函数在所有训练样本上的总和或平均值，用于在训练过程中评估模型的整体性能。

例如，linear regression中的均方误差代价函数：
$$
J(\vec w,b) =\frac 1 {2m} \sum_{i=1}^m (f_{\vec w,b}(x^{(i)})-y^{(i)})^2
$$
**Gradient Descent：**

梯度下降算法，即重复下面的过程，后续详细介绍

**repeat	 {**
$$
w_j=w_j-\alpha \frac \partial {\partial w_j}J(\vec w,b) \\
b = b - \alpha \frac \partial {\partial b}J(\vec w, b)
$$
**}**

机器学习所做的事都是对不完全确定的事的预测，所以即使像chatgpt那样根据你提问的所有的条件产出的结果也是不知道正确答案的，但它可以通过他的gpt模型预测出最符合条件的结果，这样的结果往往从感官上看是不错的，但我们始终要明白，从始至终机器学习或者大模型尝试给出答案的问题都是，从整体上看，没有绝对正确答案的问题。

parameters: w,b 参数，或者叫coefficients,weights

还需要一个东西来决定模型的优劣，或者说还需要建立一个价值衡量体系来判断模型是否符合要求或者是否是最好的。可以考虑去衡量model与training sets的拟合程度。可以使用某个确定函数作为衡量体系来判断，这个函数是什么不确定，但是可以暂且命名为cost function，他的输入结果是模型的参数，输出结果的大小可以用来衡量这个模型与训练集的拟合程度。比如说我给定一个cost function，然后来调整我的model（也即调整w,b参数，调整模型就是调参），会得到很多cost function的结果，哪个结果最小（anyway，符合我们的价值取向就行）哪个被调模型就是最拟合的（最好的）。

因此，比较容易想到的是使用均方误差来作为cost function，因为均方误差越小也就意味着模型和数据集的拟合程度更好，这是非常直观的结论，和最容易想到的方案。因此，均方误差可以作为我们对于模型的价值衡量标准（体系）。

现在数据（训练集），模型（参数还不确定），衡量标准都有了，我们现在要做的就只有一件事，也即不断地调整参数，然后根据衡量标准得到这个参数条件下的模型效果，找出最好的模型效果，那这个参数下的模型就是我们想要的，或者说我们能做到的最好的。

因此，其实我们要做的事归结到最后就是，在以w,b这些参数作为自变量的cost fucntion中找到它的最小值。

从数学角度，可以通过解析法直接一步得到这个cost function的最小值吗？也即解析解，似乎不可能。

但有聪明的人另辟蹊径，通过一种其命名为gradient descent（梯度下降）的算法，可以求出这个函数的局部最小值。



> 梯度下降算法是对任何函数都可以找到局部最小值吗？还是说有什么条件吗？ 

梯度下降算法是一种优化算法，它通过迭代的方式来寻找一个函数的局部最小值。虽然梯度下降算法在很多情况下都非常有效，但它是否能找到局部最小值，以及它找到的是否是全局最小值，取决于几个因素：

1. **函数的性质**：梯度下降算法适用于凸函数（convex functions），在这类函数上，它可以保证找到全局最小值。对于非凸函数（non-convex functions），梯度下降可能只能找到局部最小值，而且具体能找到哪个局部最小值可能取决于算法的初始点。

2. **初始点**：梯度下降算法的起始位置可能会影响算法最终找到的是哪个局部最小值，特别是在非凸函数上。不同的初始点可能会导致算法收敛到不同的局部最小值。

3. **学习率**：学习率（learning rate）决定了在每一次迭代中参数更新的幅度。如果学习率过高，可能会导致算法在最小值附近震荡，甚至发散；如果学习率过低，算法可能会收敛得非常慢，或者在达到最小值之前就停止了迭代。

4. **局部最小值和鞍点**：在高维空间中，梯度下降算法可能会遇到鞍点（saddle points），即在某个方向上是最小值，而在另一个方向上是最大值的点。在这种情况下，梯度下降算法可能会停止在鞍点，而不是局部最小值。

因此，梯度下降算法可以在很多情况下找到局部最小值，但是否能找到，以及找到的最小值的性质，依赖于上述因素。在实际应用中，为了提高梯度下降算法找到全局最小值的可能性，可以采取多种策略，如使用多个初始点、调整学习率、使用动量等优化方法。



# Batch Gradient Descent

梯度下降算法的核心思想就是对于一个以w,b作为参数的cost function，我通过不断地调整w,b使得cost function更小，而我调参（对于一个参数而言）总的来说只有两种方式，要么让这个参数w变大一点点（这里的一点点到底是多少点，谁也不知道，也没法计算出来，所以只能瞎给定一个数，具体是什么还不清楚，但不妨碍我们先给这个”一点点“取个名字，学习率），要么让这个参数w变小一点点，那到底变大还是变小？

虽然我们不知道一点点是多少点，但是变大变小还是可以知道的，这个到底变大还是变小就由cost function对这个参数的偏导决定，想象这个cost function是一个开口向上的二次函数，当我取对称轴右侧的某个w时，那function对该点w的偏导（或者说导数，再或者说斜率）肯定大于0吧（反正就是个“正数”，一定要记住这个正数，后面要考），这就是我想要的知道的事，到底是正数还是负数，当偏导大于0时，我就让这个w减去这个正数，得到的新值（后文称$w_0$)肯定比原来的w小，也即在原先的w的左侧的，与此同时，如果我还尽可能保证“在左侧”的同时还不超过对称轴，那是不是意味着我得到的这个$w_0$对应的cost function的值一定比原先的w对应的y值更小啊，也就是更靠近cost function的最小值了？这不就实现了调参的目的了吗？让我的cost function值尽可能地小。

这就是通过求某个w处的偏导得出一个正数或者负数，这里的正数或者负数在梯度下降算法中常常被抽象为“方向”这个词，得知这个偏导值的正负也就相当于得知了移动的方向，如何移动呢？用原先的w减去这个偏导值，比如说这个偏导值为正，新的$w_{0}$小于原先的w，在数轴上的表现就是，新的$w_{0}$在w的左侧，就相当于是对于抽象的参数w，他被左移了，通过不断地“移动”参数，得到最小cost function时的参数，就是所谓意义上的调参。

那这个“如果我可以尽可能保证“在左侧”的同时还不超过对称轴”怎么实现？学习率啊，只要我学习率够小，比如说0.000001，再用这么小的值乘上那个“正数”（考到了吧？）是不是就能“大概率”保证“在左侧”且“不超过对称轴”，那这样不就调优了吗？目的达成了呀。

然后我反反复复地重复这个求偏导，移动参数的过程，最后会因为偏导值越来越趋向于0，导致移动的步长也趋向于0了，也就是参数的变化越来越趋向于0了，达到了一种收敛的状态，也就没必要再调参了，整个过程也就结束了，此时的cost function就是最小值了，也即最优。

所以，来个总结吧，梯度下降的核心思想就是，通过对参数求偏导得出是把参数调大还是调小，在学习率的合理瞎设置下，不断地迭代这一调大调小的步骤，总会让cost function趋于一个局部最小值。cost function足够小，说明咱的模型（也就是函数）跟历史数据（训练集）就足够拟合了呀。

假如说我的这个模型就是一个预测天气的模型，且在我不断努力地调大调小最终得到一个小到几乎为0的cost function时，我再跟你说明天肯定下雨，你不会还跟我杠吧？引用某某人的话就是：我已经这么成功了，你都不相信我...

最后说这一段只是想再次强调机器学习的本质，他只是在对历史的记录或者数据找规律，对“正确答案”进行预测，而不是直接求出问题的正确答案，或者说解析解。然而，可能从哲学角度思考，人类社会所自定义的正确答案，其实就是一种“正确答案”吧。

# Vectorize

可以充分利用物理硬件的优势进行更快的运算，同样是向量的点乘，矢量化可以做到并行进行每个向量元素的相乘，简单理解是这样。

在机器学习中，矢量化（Vectorization）是一种编程技术，它涉及将操作从对单个数据点的处理转变为对整个数据集的同时处理。这通常意味着使用数组、向量和矩阵（即高效的数据结构，如NumPy数组）来表示数据和计算，从而使得能够通过线性代数运算而不是逐个元素的循环来执行这些操作。

**矢量化的重要性：**

1. **性能提升**：矢量化可以显著提高计算效率和速度。计算机硬件（特别是现代CPU和GPU）被设计为能够高效地执行矢量和矩阵运算。通过利用这些硬件优化，矢量化的操作通常比等效的非矢量化（如循环遍历数组）的操作要快得多。
2. **代码简洁**：使用矢量化的方法可以使代码更加简洁、易读。它减少了代码行数，提高了编写和维护代码的效率。
3. **库和框架的兼容性**：大多数现代机器学习和数值计算库（如NumPy、Pandas、Scikit-learn、TensorFlow等）都是针对矢量化操作优化的。使用这些库的矢量化操作可以确保代码获得最佳性能，并与数据科学生态系统中的标准工具兼容。
4. **减少循环**：在传统的编程实践中，针对大数据集的计算通常涉及嵌套循环，这在执行上既慢又低效。矢量化消除了这些显式循环，使得操作能够更加高效地在底层执行。

**矢量化的例子：**

- **非矢量化**：使用Python列表和循环逐个元素地计算两个列表的元素和。
- **矢量化**：使用NumPy数组并直接相加两个数组，得到相同的结果，但效率更高。

总的来说，矢量化在机器学习中非常重要，因为它可以提高计算效率，简化代码，并使得处理大规模数据集变得更加可行。

# Feature Engineering

特征工程（Feature Engineering）是数据预处理和机器学习中的一个重要步骤，指的是使用领域知识选择、修改和创建新的特征（Feature）的过程，以提高机器学习模型的性能。简而言之，特征工程是将原始数据转换成更好地表示潜在问题的形式，以帮助算法在预测时更准确。

特征工程包括多个步骤和方法：

1. **特征选择（Feature Selection）**：从原始数据中选择最相关、最有信息量的特征，以减少维度并改善模型性能。这可以通过各种统计测试、模型或算法（如决策树、正则化方法等）来实现。
2. **特征提取（Feature Extraction）**：将原始数据转换或缩减为更有用的特征集。这常见于图像处理、文本处理或维度降低技术，如主成分分析（PCA）。
3. **特征构造（Feature Construction）**：基于现有的数据特征，通过组合、改变或创建新的特征来提供更多信息。这可能包括算术运算、多项式特征创建、交互特征的生成等。
4. **特征转换（Feature Transformation）**：应用数学变换来改变特征的分布或关系，如对数转换、标准化（z-score）、归一化等，使模型更容易理解和处理数据。

通过有效的特征工程，可以显著提高模型的准确性和性能，因为好的特征可以提供更多的信息，减少噪声，并改善数据的表示方式，从而帮助模型更好地学习和做出预测。

# Feature Scaling

特征缩放，是特征转换中的一种形式，类似于统计中的标准化，均值归一化。

在机器学习中，许多算法（如支持向量机（SVM）、K-最近邻（KNN）、逻辑回归、梯度下降等）在处理时会受到特征尺度的影响。如果特征的尺度差异很大，高范围的特征可能会对结果产生更大的影响，导致模型性能不佳。

特征缩放常见的方法包括：

1. **最小-最大缩放（Min-Max Scaling）**：将所有特征缩放到[0, 1]区间内或任何其他指定区间。这通过从每个特征中减去最小值并除以最大值和最小值之差来实现。
2. **标准化（Standardization）**：将特征的值转换为具有0均值和单位方差的形式。这通过从每个特征中减去其平均值并除以其标准差来完成。这种方法对于假设数据应呈正态分布的算法特别有效。
3. **正则化（Normalization）**：常指将每个样本的特征向量调整到单位范数（如L1、L2范数），使其在不同的尺度下更容易比较。

特征缩放作为特征转换的一种，可以改善模型的收敛速度、性能和稳定性。

# Linear Regression



# Logistic Regression

sigmoid function是处理binary classification类问题的常用模型，但是这里称sigmoid function为激活函数，为什么？ 

[p32](https://www.bilibili.com/video/BV1Pa411X76s?p=32&spm_id_from=pageDriver&vd_source=d383c66eadf5006ae47659240a2eff67)重点，可能会回头看



# Regularize

$$
J(\boldsymbol w) = \frac 1 n \sum_{i=1}^n \ (\boldsymbol w^T \ \boldsymbol {x_i} + b - y_i)^2 \ +  \  \lambda \sum_{j=1}^m \ w_j^2
$$



对于某些过大的w参数，它相对于整体而言使得它对应的特征对整体结果影响太大了，这不一定是合理的，甚至有些偏颇的，所以为了降低整体的w，就是用lambda系数乘上参数的平方，使得在梯度下降过程中，参数尽可能地被调小，进而削减某些参数的影响

L1正则化（Lasso回归）

L2正则化（Ridge回归）



Elastic Net正则化

# DL (Deep Learning)

深度学习其实和线性回归，逻辑回归，是并列的关系，而不是包含的关系，它们都是机器学习模型。

机器学习是一个广泛的领域，包括了多种类型的学习方法，主要可以分为监督学习、非监督学习、半监督学习和强化学习等。监督学习和非监督学习是其中最基本和最常见的两种类型。

机器学习中，属于深度学习（Deep Learning, DL）的算法模型通常具有以下特征：

1. **多层结构**：深度学习模型通常包含多个处理层，用于从数据中自动提取和转换特征。这些层可以是全连接的、卷积的或循环的，用于处理不同类型的数据结构。
2. **自动特征提取**：与传统机器学习模型不同，深度学习模型能够自动从原始数据中学习到有用的特征表示，无需手动特征工程。
3. **非线性能力**：深度学习模型通过非线性激活函数和多层结构捕捉复杂的数据关系，使其能够近似任何非线性函数。
4. **端到端学习**：深度学习模型可以直接从原始数据到最终结果进行端到端的学习，而无需明确设计特征提取和分类器。

常见的深度学习模型及其特点包括：

1. **卷积神经网络（Convolutional Neural Networks, CNNs）**：
   - **特点**：特别适用于图像处理，通过卷积层自动从图像中提取特征，具有参数共享和局部连接的特性，减少了模型的参数数量和计算量。
   - **应用**：图像分类、目标检测、图像分割等。
2. **循环神经网络（Recurrent Neural Networks, RNNs）**：
   - **特点**：擅长处理序列数据，如时间序列或自然语言，能够捕捉时间序列数据中的时间动态特征。
   - **应用**：语言模型、文本生成、语音识别等。
3. **长短期记忆网络（Long Short-Term Memory, LSTM）**：
   - **特点**：一种特殊的RNN，能够解决传统RNN在处理长序列时的梯度消失或爆炸问题，有效地捕获长期依赖关系。
   - **应用**：序列预测、自然语言处理、语音识别等。
4. **Transformer和自注意力模型（Self-Attention Models）**：
   - **特点**：依赖自注意力机制，能够同时处理序列中的所有元素，捕捉不同位置之间的依赖关系，改善了对长距离依赖的处理。
   - **应用**：自然语言处理（如BERT、GPT系列）、机器翻译、文本摘要等。
5. **生成对抗网络（Generative Adversarial Networks, GANs）**：
   - **特点**：由一个生成网络和一个判别网络组成，通过对抗过程训练，能够生成逼真的数据样本。
   - **应用**：图像生成、图像编辑、数据增强等。

除了线性回归、逻辑回归和深度学习之外，机器学习领域还有许多其他的算法模型，这些模型可以应对不同类型的数据和问题：

1. **决策树（Decision Trees）**：一种用于分类和回归的模型，通过从数据中学习决策规则来预测目标变量。

2. **随机森林（Random Forests）**：基于决策树的集成学习方法，通过构建多个决策树并对它们的预测结果进行汇总来提高预测准确度。

3. **支持向量机（SVM, Support Vector Machines）**：一种强大的分类模型，旨在找到不同类别之间的最佳边界。

4. **K-最近邻（K-NN, K-Nearest Neighbors）**：一种基于邻近算法的模型，它通过查找一个样本点的K个最近邻居的标签来预测该样本的标签。

5. **朴素贝叶斯（Naive Bayes）**：一种基于贝叶斯定理的简单概率分类器，尽管简单但在某些类型的数据集上效果非常好。

6. **集成学习方法（如AdaBoost, Gradient Boosting Machines, XGBoost等）**：这些算法通过组合多个弱学习器的预测来构建一个强学习器，以达到提高模型性能的目的。

==在这些模型中，被广泛认为属于“大模型”的主要是深度学习模型==，特别是那些具有大量参数和复杂结构的神经网络模型，如卷积神经网络（CNNs）、循环神经网络（RNNs）、长短期记忆网络（LSTMs）和Transformer模型等。这些模型因为参数众多、结构复杂，通常需要大量的数据来训练，并且在训练过程中需要较高的计算资源，如GPU或TPU。这些模型在图像处理、自然语言处理、语音识别等领域取得了显著的成果。

# Neural Network

一种非传统的机器学习算法模型。

**传统机器学习算法**：如逻辑回归、决策树、SVM等，通常结构更简单，参数更少，适用于更具解释性的任务或者数据量较小的场景。

> 神经网络概念和深度学习概念之间的区别和联系

神经网络和深度学习之间的联系和区别可以从它们的定义、历史发展、应用和技术特点等方面进行阐述：

**联系：**

1. **基础和发展**：深度学习是基于神经网络的一种发展，尤其是当神经网络的层数增加时，即成为“深层”神经网络。深度学习算法使用这些深层神经网络来学习数据的高级特征和表示。

2. **工作原理**：无论是传统的浅层神经网络还是深度神经网络，它们都模拟了人脑的工作方式，通过一系列的神经元和连接来处理信息。深度学习通过增加网络的深度（即层数），提高了模型的学习能力和复杂性。

3. **学习目标**：神经网络和深度学习模型都旨在从大量数据中学习模式和特征。它们通过调整内部参数（权重和偏差）来最小化损失函数，从而不断改善其性能。

**区别：**

1. **层数和复杂性**：
   - **神经网络**：通常指的是浅层网络，如单层或双层前馈网络，它们的能力有限，主要用于较简单的模式识别或数据分类任务。
   - **深度学习**：指的是具有多个隐藏层的神经网络，这些多层网络能够捕捉更深层次的数据结构和特征，适用于更复杂的任务，如图像识别、语言处理等。

2. **功能和能力**：
   - **浅层神经网络**：能力较为有限，主要解决一些线性可分或相对简单的问题。
   - **深度神经网络**：能够处理非线性和高度复杂的问题，如自然语言理解、复杂图像处理等。

3. **数据和资源需求**：
   - **传统神经网络**：通常对数据和计算资源的需求较小。
   - **深度学习模型**：需要大量的训练数据和显著的计算资源（如高性能GPU），这是因为深度学习模型含有大量的参数和复杂的网络结构。

4. **发展历史**：
   - 神经网络的概念和初期模型早在深度学习之前就已存在，但由于限制在数据量、计算能力等方面，其应用受到了限制。
   - 深度学习的兴起依赖于大数据的可用性、计算能力的提升（特别是GPU的发展）以及训练深层网络的新算法（如反向传播、Dropout、ReLU等）。

简而言之，深度学习可以视为神经网络概念的一个扩展和深化，它利用了更深、更复杂的网络结构来处理数据，从而能够解决更复杂的问题。深度学习的成功在很大程度上推动了人工智能领域的发展和应用。

# Hidden Layer

# Activation Value

# Activation Functin

激活函数

# Sigmoid

用于二分类问题中输出层拟合target label的激活函数：
$$
f(x)=\frac 1 {1+e^{-z}}
$$




# Forword Propagation

前向传播，属于正常的推理逻辑，使用第0层，也即输入层，的输入，通过激活函数来计算第一层中每个单元的标量输出，最后合成第一层的向量输出，即$\vec a^{[1]}$​。以此类推得到最后的输出层的结果。

前向传播（Forward Propagation）是神经网络中的一种算法，用于从输入层通过隐藏层（如果有的话）传递信息，直到输出层，并最终得到输出值。这是神经网络进行预测或分类决策的过程。前向传播的步骤通常包括以下几个方面：

1. **输入层**：网络接收输入特征。这些输入特征构成了网络的起始层。
2. **权重和偏置**：每个输入特征将与相应的权重相乘，权重表示输入特征对于输出的重要性。每个神经元还可能添加一个偏置值，以进一步调整输出。
3. **激活函数**：每个神经元的加权输入和偏置的总和通常通过一个激活函数进行处理，以引入非线性因素，使得网络能够学习和表示复杂的数据模式。常用的激活函数包括ReLU、Sigmoid、Tanh等。
4. **隐藏层**：处理后的信号传递到一个或多个隐藏层（如果存在的话），在这些层中，它们再次被加权和偏置，然后通过激活函数。这个过程可以增加网络的深度和复杂性。
5. **输出层**：最后，经过所有隐藏层的处理后，信息到达输出层。在输出层，数据经过最后一轮加权和激活（取决于问题类型，如回归或分类），以产生最终的预测结果。

前向传播的目的是得到一个预测输出，并将这个输出与实际输出进行比较，以计算损失函数。这个损失值然后在随后的反向传播过程中用来更新网络中的权重和偏置，以改善模型的预测性能。简而言之，前向传播是神经网络生成预测并评估其性能的第一步。

# Tensorflow

一个ML框架。

```python
input = np.array([[200.0, 17.0]])
layer_1 = Dense(units=3, activation='sigmoid')  # Dense初始化一个units神经元为3，activation激活函数为'sigmoid'的全连接层对象
output = layer_1(input) // 通过全连接层对象layer_11得到输出结果
```

矩阵是 tensorflow的数据表示方式：

例如矩阵：
$$
\left[ \begin{array}{c}200 & 17\end{array} \right]
$$
在tensorflow中的表示就是`np.array([[200,17]])`

而矩阵：
$$
\left[ \begin{array}{c}200 \\ 17\end{array} \right]
$$
在tensorflow中的表示就是`np.array([[200],[17]])`

内层的括号表示一行。



构建一个neural network architecture:

```python
layer_1 = Dense(units=3, activation='sigmoid')
layer_2 = Dense(units=1, activation='sigmoid')
model = Sequential([layer_1, layer_2])
x = np.array([
    [200.0, 17.0],
    [120.0, 5.0],
    [425.0, 20.0],
    [212.0, 18.0]
])  # x 为 特征
y = np.array([1,0,0,1]) # y 为标签值或者说实际值，实际结果
model.compile(...)  # 这里还未完成，后续补充，这里需要指定loss function
model.fit(x,y)  # 自动构建neural network
yhat = model.predict(new_feature_var)  # 这里得到的yhat为模型预测的结果
println("自动构建模型后，使用该模型预测出来的值是: ",yhat)
```



# BCE Loss Function

逻辑回归问题中最常使用的Loss function，由最大似然估计推导。

Binary Cross-Entropy (BCE), 也称为对数损失，是用于二分类问题的损失函数。在二分类问题中，模型的目标是预测一个事件发生的概率，输出值通常介于0和1之间。Binary Cross-Entropy 损失函数衡量的是模型预测概率与实际标签之间的不一致性。

具体来说，如果我们有真实标签 $y$ （其中 $y$ 可以是0或1，表示两个类别）和模型预测为某一类别的概率 $p$，那么 Binary Cross-Entropy 损失可以定义为：
$$
BCE=\frac {−1} N ∑_{i=1}^N [y_ilog⁡(p_i)+(1−y_i)log⁡(1−p_i)]
$$


其中：

- $N$ 是样本数量。
- $y_i$ 是第 $i$ 个样本的实际标签。
- $p_i$ 是模型预测第 $i$ 个样本为正类的概率。
- $log$⁡ 是自然对数。

这个损失函数的值越低，表示模型的预测结果与实际值越接近，即模型的性能越好。Binary Cross-Entropy 直观地惩罚了预测概率与实际标签之间的差异。特别地，当实际标签为1时，损失函数关注于模型预测为正类的概率；当实际标签为0时，损失函数则关注于模型预测为负类的概率。通过最小化这个损失函数，可以训练出更准确的二分类模型。

# MLP

多层感知机（MLP, Multilayer Perceptron）和神经网络（Neural Networks）这两个术语经常被互换使用，但它们之间有一些细微的差别。在最广泛的意义上，多层感知机是神经网络的一种特定类型，更准确地说，是前馈神经网络的一种。

**多层感知机（MLP）**：
- 多层感知机是一种前馈神经网络（Feedforward Neural Network），意味着数据在网络中只向一个方向流动：从输入层，通过一个或多个隐藏层，最终到达输出层。
- 它包含至少三层节点：一个输入层，一个或多个隐藏层，以及一个输出层。
- 每个节点（除了输入层的节点）都是一个带有**非线性激活函数**的神经元，这是MLP能够学习非线性模型的关键。
- MLP通过一个称为反向传播（Backpropagation）的训练过程进行学习，这个过程使用梯度下降（或其他优化算法）来最小化预测值和真实值之间的差异。

**神经网络（Neural Networks）**：
- 神经网络是一个更广泛的术语，**它包含了多层感知机以及其他类型的网络结构，如卷积神经网络（CNNs）、递归神经网络（RNNs）等**。
- 神经网络可以是前馈的，也可以是反馈的（如在某些RNNs中，信息可以在网络中向后传播），可以处理各种不同类型的任务，从图像和语音识别到自然语言处理和超越。

因此，可以说多层感知机是神经网络的一个子集。所有多层感知机都是神经网络，但不是所有神经网络都是多层感知机。神经网络的概念更加广泛，包括了各种结构和用途的网络，而多层感知机特指那种结构相对简单、流程单向的网络模型。



# ReLu

激活函数的一种，隐藏层的最常见激活函数：
$$
f(x)=max(0,x)
$$


# Softmax Regression

Softmax回归，通常用于多类分类问题，是逻辑回归的一个推广。在逻辑回归中，我们预测两个类别的概率分布；而在Softmax回归中，我们可以预测多个类别的概率分布。这种方法特别适用于类别之间是互斥的情况，即每个实例都只能属于一个类别。

假设我们有一个分类问题，其中类别的总数为 K。对于给定的输入特征 $x$，Softmax回归模型首先计算每个类别的得分（也称为逻辑），通常是特征 $x$ 和该类别对应权重 $w$ 的线性组合，加上偏置项。

然后，Softmax函数被应用于这些得分，以获得每个类别的概率。对于第 $k$ 类，概率 $P(y=k∣x)$ 计算如下：
$$
P(y=k∣x)={e^{\boldsymbol w_k^T \boldsymbol x+b_k}\over∑_{j=1}^K e^{\boldsymbol w_j^T \boldsymbol x+b_j} }
$$
这里，$e^{ \boldsymbol w_k^T \boldsymbol x+b_k}$ 是第 $k$ 类的得分的指数，分母是所有类别得分的指数和，确保所有类别的概率之和为1。

Softmax回归广泛应用于多类别的分类问题，如手写数字识别、图像分类等领域。由于其输出可以解释为概率分布，因此它不仅可以用来预测每个实例最可能属于哪个类别，还可以提供关于这些预测的不确定性度量。



tensorflow实现：

```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model = Sequential([
	Dense(units=25, activation='relu')
	Dense(units=15, activation='relu')
	Dense(units=10, activation='softmax')
])  # build model architecture
 
model.compile(loss=SparseCategoricalCrossentropy())
model.fit(X, Y, epochs=100)
```



改良版实现，核心思想是，不要提前计算中间值，而是直接把整个计算过程交给tensorflow自己进行优化，这样得出的结果会更精确：

```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model = Sequential([
	Dense(units=25, activation='relu')
	Dense(units=15, activation='relu')
	Dense(units=10, activation='linear')  # 注意此处的差别，使用linear而不是sigmoid意味着不在此处产生中间值
]) // build model architecture
 
model.compile(loss=SparseCategoricalCrossentropy(from_logits=True))  # sigmoid放到这里 
model.fit(X, Y, epochs=100)
logits = model.(X)
f_x = tf.nn.softmax(logits)
```



# Multi-label Classification Problem

理解：

输出（target label）只有一个，但是可能是1到10中的某一个；输出有三个，第一个输出可能是1到10中的某个，第二个输出可能是1或者2，第三个输出可能是1或者-1中的某一个。

前者是多（二）分类问题，而多标签多分类问题属于后者。



# Adam Algorithm

Adam算法是一种用于深度学习优化的算法，全称为Adaptive Moment Estimation。它结合了两种不同的梯度下降优化算法的优点：即RMSProp和Momentum。Adam算法旨在通过计算梯度的一阶矩估计和二阶矩估计来调整每个参数的学习率，使其自适应。

在Adam算法中，每个参数的学习率都有一个自适应调整机制，这意味着每个参数都保持其自己的学习率，这有助于解决梯度消失或梯度爆炸的问题，这些问题通常会影响深度神经网络的训练。

具体来说，Adam算法会跟踪梯度的一阶矩（即均值）和二阶矩（即未中心化的方差），并将它们组合到参数更新中，这样做可以帮助算法在不同的参数和训练阶段保持适当的步长，从而加快收敛速度并提高训练过程的稳定性。



tensorflow实现：

```python
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model = Sequential([
	Dense(units=25, activation='sigmoid')
	Dense(units=15, activation='sigmoid')
	Dense(units=10, activation='linear')
])

model.compile(optimizer=Adam(learning_rate=1e-3),loss=SparseCategoricalCrossentropy(from_logits=True))
```



# Dense Layer 

Dense Layer通常就是我们所说的全连接层（Fully Connected Layer）。在神经网络中，一个全连接层的每个神经元都与前一层中的所有神经元连接。这种层常用于神经网络的后几层，以整合之前层的特征并进行最终的判断或预测。

在全连接层中，每个输入都会通过权重矩阵与每个输出相乘，通常还会加上一个偏置项，然后通常会通过一个激活函数（如ReLU、Sigmoid或Tanh）来引入非线性。这种结构使得全连接层能够学习输入数据中的非线性组合特征。



# Cross Validation Set

交叉验证集，也可以叫validation set，development set，dev set等等。

训练集用来比较同一模型中不同参数的优劣，即寻找到指定模型下的最优参数。

交叉验证集用来比较不同模型（不同的degree of polynomial，$\lambda$）的优劣，即这些不同的模型都已经拥有了各自的最优参数后，分别对这些不同模型直接应用dev set中的数据，进而来比较标签数据和这些不同模型预测出的结果的cost function(这里已经不需要使用梯度下降对w,b调优了），而是直接验证模型的泛化能力，通过cost function来比较出最好的模型。

测试集（test set）是真正地去验证上一个过程测试出来的最好的模型的泛化能力的过程。



# High Bias

高偏差，可以理解为欠拟合（underfit）。



# High Variance

高方差，可以意味着过拟合（overfit）。

 

# Learning Curves

 

#  Training Procedure(not term)

![image-20240222181921373](C:\Users\19535\AppData\Roaming\Typora\typora-user-images\image-20240222181921373.png)





# Transfer Learning



# Precise And Recall

**召回率（Recall）**反映了模型捕获相关样本的能力。想象一下，有一个用于检测疾病的医疗测试。在这个例子中，召回率会回答这个问题：“在所有真正患有疾病的人中，有多少人被模型正确地诊断出来了？”

为了更形象地理解，假设在100个实际患有某种疾病的人中，模型只识别出了80人。在这种情况下，召回率是80%（因为有20%的真实病例没有被识别，被遗漏了）。在医疗诊断的背景下，一个高召回率非常重要，因为你不希望漏掉任何真正的病例——即使这意味着某些健康的人会被误诊为病人（这会降低精确率，但提高召回率）。

在不同的应用中，召回率的重要性不同。在某些情况下（如垃圾邮件检测），错过一些垃圾邮件（低召回率）可能是可以接受的，如果这意味着减少了重要邮件的误标（即提高精确率）。而在其他情况下（如癌症筛查），高召回率至关重要，因为错过真正的病例可能会有生命危险。

==实际上，==精确率描述的是在所有模型预测为正例的结果中，确实为正例的比例占到了多少，这展现了一个模型在预测上的精确度，这是针对模型本身的能力，并不涉及到具体的应用，而召回率是指在所有客观的正例中，模型预测出来的正确的正例的比例，所以可能会发生这种情况，比如说，正确的正例可能没多少，但是模型把很多负例都预测成了正例，这会导致模型的精确度不高，但是因为模型把大部分例子都预测成了正例，即使他的精确度不高，但是这些预测出的正例也足以涵盖绝大部分真正的正例了，因此模型预测出的正例在整个真正的正例中比例是很高的，这就造就了很高的召回率，因此可以发现，即使精确度不高但是召回率却可以很高，总结来说就是通过尽可能地把负例也预测为正例，这样就可以保证在所有的结果中，正例被尽可能地挑选出来了，导致了高召回率，正如上例所示，这种精确率不高但是召回率很高的模型，在应用层具备了一定的意义。

# F1 score

 F1分数（F1 Score）是一个在二分类和多分类问题中常用的性能度量，特别是在数据集不平衡的情况下。它是精确率（Precision）和召回率（Recall）的调和平均值。在解释召回率之前，我们先定义F1分数和相关术语：

- **精确率（Precision）**：是模型预测为正类（如“有病”、“是垃圾邮件”等）的样本中，实际为正类的比例。公式为：$Precision={TP \over TP+FP}$ 其中TP是真正例（true positives），FP是假正例（false positives）。

- **召回率（Recall）**：也称为真正例率，是实际为正类的样本中，模型正确预测为正类的比例。公式为：$Recall={TP \over TP+FN}$ 其中FN是假负例（false negatives）。

- **F1分数（F1 Score）**：是精确率和召回率的==调和平均值==，给出了这两个指标之间的平衡度量。公式为：
  $$
  F1=2×{Precision×Recall \over Precision+Recall}
  $$



# Decision Tree

决策树是一种常见的机器学习方法，用于分类和回归任务。它是一种树形结构，其中每个内部节点代表一个属性上的测试，每个分支代表测试的结果，每个叶节点代表一个类别（在分类树中）或一个连续值（在回归树中）。

**决策树的组成：**

1. **根节点（Root Node）**：包含整个样本集，是决策树的起点。
2. **内部节点（Internal Nodes）**：表示一个特征或属性。
3. **分支（Branches）**：代表决策规则，是从一个节点分裂到下一个节点的路径。
4. **叶节点（Leaf Nodes）**：代表决策结果，即最终的分类或预测值。

**工作原理：**

决策树的构建过程主要是选择最优特征并决定分裂点。这个过程基于特定的算法，例如ID3、C4.5、CART等。这些算法通常采用贪心算法，从根节点开始，递归地选择最优特征进行分裂，直到满足某个终止条件，比如：

- 所有的样本都属于同一类别。
- 没有剩余的特征可以用来进一步划分样本。
- 达到预定的最大深度。
- 分裂后的节点增益小于某个阈值。

**决策树的优缺点：**

**优点**：

- 易于理解和解释，可以可视化分析。
- 不需要对数据进行大量预处理，如创建虚拟变量或删除空值。
- 能够同时处理数值型和类别型数据。
- 可以处理多输出问题。

**缺点**：

- 易于过拟合，特别是当树很深时。
- 可能会创建偏见较大的树，如果某些类占主导地位。
- 决策树模型可能不稳定，数据中微小的变化可能导致生成完全不同的树。
- 某些类型的问题（如XOR、奇偶校验或多路复用器问题）可能难以学习。

由于这些特点，决策树在各种领域都有广泛的应用，从金融风险分析到医疗诊断，再到制造流程控制等。在实际应用中，为了克服单一决策树的缺点，经常会使用**随机森林**或**梯度提升树**等集成方法来提高预测准确性和稳定性。



# Information Gain

信息增益（Information Gain）是决策树等机器学习算法中用来选择分裂属性的一个重要指标，它基于熵（Entropy）的概念。信息增益衡量的是在知道某属性的信息之后使得类别信息的不确定性减少的程度。换句话说，信息增益表示由于属性$A$的出现而使得数据集$D$的熵减少了多少。

如果我们有一个数据集$D$，其熵为$H(D)$，这个熵反映了当前数据集的不确定性。假设数据集$D$根据属性$A$的不同值被分割成了若干个子集$D_1$, $D_2$, ..., $D_n$，每个子集的熵为$H(D_i)$。属性\(A\)对数据集$D$的信息增益$IG(D, A)$​定义为：
$$
IG(D, A) = H(D) - \sum_{i=1}^{n} \left( \frac{|D_i|}{|D|} \cdot H(D_i) \right)
$$
其中：

- $H(D)$是数据集$D$的原始熵，
- $|D_i|$是分割后子集$D_i$中的样本数，
- $|D|$是原始数据集$D$中的样本总数，
- $H(D_i)$是子集$D_i$的熵。

熵函数$H(D)$可以是：
$$
p_0 = 1- p_1 , \quad H(p_1)=-p_1 \log_2(p_1)-p_0\log_2(p_0)
$$
其中，$p_1$为满足属性$A$的比例。

信息增益越大，说明属性$A$使得数据集$D$的不确定性减少得越多，因此这个属性对于分类来说越重要。在构建决策树时，通常会选择信息增益最大的属性来进行分裂。

# One-hot encoding

One-hot encoding（独热编码）是将分类变量转换为机器学习模型可以理解的形式的过程。在机器学习中，很多算法更适合处理数值数据而不是文本标签。因此，我们需要一种方法来转换这些类别标签为数值形式。

假设有一个分类特征“颜色”，它包括三个可能的值：红、绿、蓝。在one-hot编码中，我们会为每一个可能的类别值创建一个新的二进制特征（特征值为1或0）。对于“颜色”这个例子，我们将创建三个新的特征：“是红色”、“是绿色”和“是蓝色”。

- 如果一个样本的颜色是红色，那么在“是红色”这一特征下会标记为1，而在“是绿色”和“是蓝色”这两个特征下会标记为0。
- 如果另一个样本的颜色是绿色，则在“是绿色”下标记为1，在“是红色”和“是蓝色”下标记为0。
- 以此类推。

例如：

原始数据：

| 样本 | 颜色 |
| ---- | ---- |
| 1    | 红   |
| 2    | 绿   |
| 3    | 蓝   |

经过one-hot编码后：

| 样本 | 是红色 | 是绿色 | 是蓝色 |
| ---- | ------ | ------ | ------ |
| 1    | 1      | 0      | 0      |
| 2    | 0      | 1      | 0      |
| 3    | 0      | 0      | 1      |

有点：

- 使模型能够明确地理解不同的类别之间没有数学上的顺序关系。
- 改善了模型的性能和准确度。

缺点：

- 如果类别变量有许多级别，独热编码会导致维度急剧增加，这可能会导致“维度灾难”，即特征空间过大，可能会增加计算负担，也可能会导致模型过拟合。
- 丢失了类别之间的信息（如果有的话），因为独热编码假设不同的类别是独立的。

尽管存在这些缺点，one-hot encoding在处理分类数据时仍然是非常流行和有效的技术，特别是在处理线性模型、神经网络等机器学习算法时。对于具有大量类别的特征，人们可能会考虑使用其他技术，如嵌入(embedding)或二进制编码，以减少维度的影响。

# Regression Tree



# Tree Ensembles



# Random Forest Algorithm



# XGBoost



# Clustering Algorithm

聚类算法是一种数据分析方法，用于将一组对象根据相似性分组，使得同一组（称为一个簇）内的对象彼此之间更相似，而与其他组中的对象相比则差异更大。它是一种无监督学习方法，是统计数据分析中使用的常见技术。

聚类算法在许多不同领域中都有应用，包括机器学习、模式识别、图像分析、信息检索和生物信息学。与分类算法不同，分类算法是基于标记数据将数据分类到预定义的组中，而聚类算法则是识别数据点之间的相似性，并将相似的数据点分组在一起。

聚类算法有几种不同的类型：

1. **K-均值聚类（K-Means Clustering）**：这是最简单和最流行的聚类算法之一。它的目标是将 n 个观测值划分为 k 个簇，每个观测值属于最近的平均值所代表的簇。这导致数据空间被划分成Voronoi单元。

2. **层次聚类（Hierarchical Clustering）**：这种方法通过一系列的合并（聚合方法）或分裂（分裂方法）构建簇的层次结构。结果通常以树状图（dendrogram）呈现。

3. **DBSCAN（基于密度的空间聚类应用与噪声）**：这种算法将簇视为高密度区域，由低密度区域分隔开。它可以找到任意形状的簇，并且甚至能识别出不属于任何簇的点（被视为噪声）。

4. **均值漂移聚类（Mean Shift Clustering）**：这是一种基于质心的算法，通过更新候选质心为给定区域内点的均值来工作。它用于定位给定离散数据样本的密度函数的最大值。

5. **谱聚类（Spectral Clustering）**：该技术利用相似性矩阵的特征值来降维，然后在较低维度中进行聚类。当簇的形状非常不规则或比球形复杂得多时，经常使用此方法。

6. **亲和力传播（Affinity Propagation）**：它通过在样本对之间发送消息直至收敛来创建簇。与K-均值不同，它不需要在运行算法之前确定或估计簇的数量。

选择聚类算法基于数据的具体要求和期望的结果。每种类型的算法都有其自身的优点和缺点，并适用于不同类型的数据集和不同的聚类任务。

# K-means Algorithm 

 Repeat {

1. figure out which cluster centroid among all of the clusters is closest to the vector $x_m$ of the feature and then mark the vector $m_x$ as the member of cluster $k$ (assuming the cluster $k$ is the most qualified)
1. calculate the average of all of the vectors in the cluster $k$ and let the average be the new centroid

}



the distortion (cost function) of k-means algorithm:
$$
J(c^{(1)},\dots,c^{(m)},\mu_1,\dots,\mu_K)=\frac 1 m \sum\limits_{i=1}^m\|x^{(i)}-\mu_{c^{(i)}}\|^2
$$
The k-means algorithm does its best to minimize the distortion.

If the distortion keep the same for a single duration, it's possible that the distortion does have converged.

![image-20240226165036203](C:\Users\19535\AppData\Roaming\Typora\typora-user-images\image-20240226165036203.png)

In the above diagram, there are three distortion value of $J_1,J_2,J_3$, which we can get the results for the same training set through the k-means algorithm. And among all of these three, we konw the min value is $J_1$ which is what we want.



How to initialize the centoiod?

1. obviously, we should hold the inequality  $K < m$ true.
2. select the $x_m$ as the centroid at random.
3. repeat this proceduce for several times(may be 50 through 100 times) util the distortion converges

