# Supervised learning

监督学习是指使用一组**已经标记的**数据来训练模型。在这个过程中，每个训练样本都是由输入特征和对应的输出标签组成。模型的任务是学习输入和输出之间的映射关系，以便当新的输入数据出现时，模型可以预测出相应的输出。监督学习的任务通常分为两种：分类和回归。分类任务的目标是预测离散的标签，而回归任务的目标是预测连续的数值。

已经标记的，是指已知了该数据的正确输出。

# Unsupervised Learning

无监督学习则与之不同，它使用的是没有标记的数据。无监督学习的目标是发现数据中的结构和模式。由于输入数据没有标签，因此无法指导模型学习特定的输出，模型需要自行找出数据的分布特性和结构。无监督学习的任务包括聚类（Clustering）、降维（Dimensionality Reduction）、关联规则（Association  Rules）等。聚类任务试图将数据分组，使得同一组内的数据彼此相似，而不同组的数据相异；降维则试图寻找数据的低维表示，以便更简洁地表达数据的特性。

监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）是机器学习中的两种主要类型。

简而言之，监督学习侧重于利用标记的数据集来预测或分类数据，而无监督学习侧重于探索未标记数据中的潜在结构。

# Classification Problem

分类问题（Classification Problem）是监督学习（Supervised Learning）中的一种，其目的是预测和判定输入数据属于哪个类别。在分类问题中，我们通常有一组已经标记的训练数据，每一个训练实例都被标记为某个类别，我们的目标是构建一个模型，这个模型可以接收新的未标记数据，然后根据学习到的数据特征将其准确地分类到一个类别中。

分类问题根据标签的不同，可以分为二分类问题和多分类问题。二分类问题是指只有两个类别，如判断邮件是否为垃圾邮件，而多分类问题是指有两个以上的类别，例如识别一张图片中的物体是猫、狗、还是车。

在实际应用中，分类算法包括但不限于逻辑回归、支持向量机、决策树、随机森林和神经网络等。分类问题广泛应用于医学诊断、图像识别、垃圾邮件检测、语音识别等领域。

# Regression Problem

回归问题（Regression Problem）是监督学习中的另一种类型，其目标是预测或估计**一个连续的输出值**。与分类问题不同，分类问题的输出是离散的标签，而回归问题的输出是连续的数值。

在回归问题中，我们有一组带有连续数值标签的训练数据，这些数值标签通常是实数。我们的目标是构建一个模型，这个模型能够基于输入数据的特征来预测一个数值。例如，我们可能想要预测一栋房子的售价、一个地区未来的温度、或者一个企业下个季度的销售额。

回归分析的方法包括线性回归、多项式回归、岭回归、LASSO回归和弹性网络回归等。这些方法试图找到输入特征和输出值之间的关系，从而可以用来预测新的输入数据对应的输出值。

回归问题在经济预测、资源消耗预测、股票价格预测等多个领域有着广泛的应用。

# Hypothesis

假设函数 $h$ :
$$
h_\theta(x) = \theta_0 + \theta_1x
$$

# cost function

square error function:
$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
$$
这里的 $\frac 1 {2m}$  而不是 $\frac 1 m$ 是为了在后续计算梯度下降时简化计算。

当我们对成本函数$J(\theta_0,\theta_1)$求偏导数以进行梯度下降时，平方项$(h_\theta(x^{(i)})-y^{(i)})^2$的导数会带来一个 2 的系数。通过在成本函数前面乘以 $\frac 1 2$，我们可以在求导时抵消这个 2，从而使得梯度的表达式更加简洁。

举个例子，成本函数对 $\theta_1$ 的偏导数是：
$$
\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} 2(h_\theta(x^{(i)}) - y^{(i)}) \cdot \frac{\partial}{\partial \theta_1} (h_\theta(x^{(i)}) - y^{(i)})
$$




其中 $h_θ(x^{(i)})$ 对 $\theta_1$ 的偏导数是 $x^{(i)}$。如果我们把这个 2 和前面的 $\frac 1 {2m}$ 相乘，2 就被抵消了，简化为：
$$
\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}
$$
这样，我们就得到了一个更简单的梯度表达式，便于计算和实现。这是数学上的一个技巧，使得算法更加易于处理。在实际应用中，因为成本函数是用来衡量误差的，这个常数系数 $\frac 1 2$ 并不会影响最优化参数 $\theta$ 的求解，因为它对所有的参数估计值都是相同的影响。

# Gradient descent

gradient descent algorithm:

repeat until convergence {
$$
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\qquad(for \quad j = 0 \quad and \quad j = 1)
$$
}



其中$\alpha$ 为learning rate。

# Hypothesis in linear regression

$$
h_\theta(x)=\theta_0+\theta_1x_1+\cdots+\theta_nx_n
$$

假设这里定义了一个$x_0=1$

那么对于：
$$
x=\begin{bmatrix}
x_0\\
x_1\\
\vdots \\
x_n
\end{bmatrix} \in \mathbb{R}^{n+1} 
\quad \text{and} \quad 
\theta =
\begin{bmatrix}
\theta_0 \\
\theta_1 \\
\vdots \\
\theta_n
\end{bmatrix}
$$
有，hypothesis:
$$
h_\theta(x)=\theta_0x_0+\theta_1x_1+\cdots+\theta_nx_n=\theta^Tx
$$

# Feature Scaling

Feature Scaling（特征缩放）是数据预处理中的一种方法，用于标准化独立变量或特征的范围。在机器学习和数据分析中，当各个特征的尺度（范围）不一致时，特征缩放变得非常重要。这是因为，许多算法的性能或结果会受到特征尺度不一致的影响，尤其是那些基于距离的算法，如 K-近邻（KNN）、支持向量机（SVM）和主成分分析（PCA）。

特征缩放的主要类型：

1. **最小-最大缩放（Min-Max Scaling）**: 这种方法将所有特征缩放到 [0, 1] 的范围内，或者是更一般的 [a, b] 范围内。公式为： $X_{scaled}=\frac {X−X_{min}} {X_{max}−X_{min}}$ 这里 $X_{max}$ 和 $X_{min}$ 分别是特征中的最大值和最小值。
2. **标准化（Standardization）**: 标准化将特征的平均值调整为 0，标准差调整为 1。公式为： $X_{standardized}=\frac {X−μ} σ$其中，μ 是特征的平均值，σ 是标准差。
3. **正则化（Normalization）**: 正则化调整数据，使得每个样本的特征向量的欧几里得长度为 1。换句话说，它将数据点投影到单位范数球上。这在文本分类和聚类中尤其有用。

为什么特征缩放很重要?

1. **加速学习**：在使用梯度下降等优化算法时，特征缩放可以帮助加快收敛速度。
2. **提高准确度**：对于许多基于距离的机器学习算法，如果一个特征的范围比另一个特征大很多，那么它可能会对距离的计算产生不成比例的影响，从而降低算法的性能。
3. **消除尺度不一致的影响**：特征缩放消除了不同量级的特征之间的尺度差异，使得所有特征被平等对待。

注意事项:

- 特征缩放应该根据具体算法和应用场景选择合适的方法。
- 在某些情况下，如决策树和随机森林等基于树的方法，特征缩放可能不是必需的，因为这些算法不是基于距离的。
- 在进行特征缩放时，重要的是要在训练集上学习缩放参数（如最小值、最大值、平均值和标准差），然后将这些参数应用于测试集，以防止数据泄露。

# learning rate

学习率（Learning Rate）是机器学习和优化理论中的一个重要概念，尤其是在神经网络的训练中。学习率决定了在梯度下降（Gradient Descent）或其他优化算法中，模型参数在每次迭代时更新的幅度。

当我们使用梯度下降等方法来最小化损失函数时，学习率决定了参数在损失函数梯度指示的方向上应该移动多远。学习率太高可能导致模型在最小值附近“跳过”，无法收敛到最优解；而学习率太低则可能导致训练过程过于缓慢，甚至在达到最小值之前就停止了训练。

因此，选择合适的学习率对于模型的训练效率和最终性能至关重要。学习率可以是固定的，也可以随着训练的进行而动态调整。有多种策略用于调整学习率，例如学习率衰减（Decay），或者更复杂的自适应学习率算法，如Adam和AdaGrad。

# Normal Equation

在机器学习和统计中，"Normal Equation"（正规方程）是一种用于求解线性回归模型参数的方法。它提供了一种直接计算模型参数（例如，权重）的方式，这些参数将最小化训练集上的成本函数。

假设你有一个线性回归模型，模型预测 $\hat y$ 与实际值 $y$ 之间的关系可以用矩阵表示为 $\hat y=X\theta$，其中 $X$ 是特征矩阵，$θ$ 是模型参数的向量。

在这个设置中，正规方程用于找到最小化成本函数的参数 $θ$ 的值。成本函数通常是均方误差（MSE）。正规方程的公式如下：
$$
\theta = (X^TX)^{-1}X^Ty
$$
$X^T$ 是 $X$ 的转置。

$(X^TX)^{−1}$ 是矩阵 $X^TX$ 的逆。

$y$ 是目标值的向量。

特点和使用场景：

1. **无需迭代**：与梯度下降等迭代方法不同，正规方程一步到位地给出了最佳解。
2. **计算成本**：对于有大量特征的数据集（例如，特征数量非常大），计算 $(X^TX)^{−1}$ 可能非常昂贵。在这种情况下，使用梯度下降或其他优化算法可能更有效。
3. **适用性**：正规方程只适用于线性模型。对于非线性模型，需要使用其他方法。
4. **数值稳定性**：在某些情况下，$X^TX$ 可能不是可逆的（或接近不可逆），这使得直接应用正规方程变得困难。在这种情况下，可以通过正则化技术来解决。

# 神经网络

Neural Network，又叫人工神经网络。

用于对函数进行估计和近似的数学模型。

# MP神经元模型

$$
t = f(W^{T}A+b)
$$

# 单层神经网络

单层神经网络，通常指的是由输入层和一个神经元层组成的最简单形式的神经网络。在这种网络中，输入直接连接到输出，中间没有隐藏层。这种单层结构通常用于实现简单的线性或非线性映射。

1. **单层神经网络的结构**：
   - **输入层**：由输入特征组成，这些特征是网络要处理的数据。
   - **输出层（神经元层）**：由一个或多个神经元组成。每个神经元对输入数据进行加权求和，然后通过一个激活函数生成输出。
2. **单层神经元的输出为向量的原因**：
   - 当我们谈论单层神经网络的“单层”时，这通常指的是单个的输出层。如果输出层包含多个神经元，每个神经元都会对相同的输入进行操作，但根据它们各自的权重和偏置产生不同的输出值。
   - 因此，如果输出层有多个神经元，网络的输出将是一个向量，其中每个元素对应一个神经元的输出。例如，如果输出层有3个神经元，那么网络的输出将是一个包含3个数值的向量，每个数值是一个神经元的输出。
3. **应用**：
   - 单层神经网络通常用于简单的任务，例如二元分类（单个输出神经元）或多类分类（输出层中每个类别一个神经元）。
   - 由于它们的结构简单，单层神经网络不能捕捉复杂的数据模式，这限制了它们的应用范围。对于更复杂的任务，通常需要使用多层神经网络（即深度学习模型）。

总的来说，单层神经网络的输出是向量的原因在于输出层可以包含多个神经元，每个神经元对输入数据进行独立的处理和输出。这种结构使得网络能够同时进行多个简单的运算，如多个二元分类或单个多类分类任务。

# 感知机

感知机类似于神经网络中的神经元（即神经网络的每个节点）。

# 多层神经网络

包含输入层，输出层和隐藏层。

每一层神经元个数不确定。

# 全连接层

在神经网络中，一个“全连接层”指的是一个网络层，其中该层的每个神经元都与前一层中的每个神经元相连。这意味着在这种层中，输入层的每个节点（神经元）都全面连接到输出层的每个节点。

全连接层在前一层的输出的基础上进行了一次$Y=Wx+b$的变化。

特点：

1. **密集连接**：每个输入节点（或神经元）都与下一层的每个节点相连。这意味着每个输入对下一层中每个神经元的输出都有影响。
2. **参数数量**：由于每个连接都有一个对应的权重，所以全连接层通常包含大量的参数。例如，如果前一层有 n 个神经元，当前层有 m 个神经元，那么全连接层就会有 $n×m$ 个权重，加上 m 个偏置。
3. **功能**：全连接层主要用于综合前面层提取的特征，并基于这些特征做出最终的预测或分类。

# 激活函数

激活函数在神经网络中起着至关重要的作用。它们是应用于神经元输出的函数，用于引入非线性特性到网络中。没有激活函数，神经网络将无法学习和模拟复杂的数据如图像、视频、音频和类似的数据。激活函数帮助神经网络解决非线性问题，使其能够进行复杂模式的学习和预测。

激活函数的关键特点：

1. **非线性**：激活函数通常是非线性的。这使得神经网络能够捕捉到输入数据中的复杂和非线性关系。
2. **帮助学习**：通过非线性转换，激活函数帮助神经网络学习从输入到输出的复杂映射。没有它们，网络无论有多少层，其实际上只能表达线性变换。
3. **决定神经元是否激活**：激活函数决定了在给定的输入下一个神经元是否应该被“激活”或输出信号。

常见的激活函数：

1. **Sigmoid或Logistic函数**：这是一个将输入压缩到0和1之间的S型曲线函数。它在早期神经网络中非常流行，尤其是用于二分类问题。
2. **ReLU（Rectified Linear Unit）**：ReLU函数提供了一个简单的非线性变换。如果输入为正，则输出该输入；如果输入为负，则输出0。这个函数在现代神经网络中非常流行，因为它比Sigmoid或Tanh函数计算上更有效，并且在训练深度网络时效果更好。
3. **Tanh（双曲正切函数）**：Tanh函数将输入压缩到-1和1之间。它与Sigmoid相似，但其输出范围是对称的。
4. **Softmax函数**：Softmax函数常用于多类分类的输出层。它将输入转换为一组表示概率分布的输出



为什么需要激活函数？

如果没有激活函数，神经网络的每一层本质上只能执行线性变换。线性方程无论如何组合，其结果仍然是线性的，这限制了网络的表达能力。非线性激活函数使得神经网络能够学习和表示更加复杂的函数，从而可以处理更复杂的数据，如图像、声音、语言等。

# Tensor

 张量，简单理解多维数据。

比如：

1. **Scaler标量（0阶张量）**：
   - 一个普通的数字，无方向，例如温度或质量。
2. **Vector向量（1阶张量）**：
   - 由数字组成的一维数组，有方向，例如速度或加速度。
3. **Matrix矩阵（2阶张量）**：
   - 数字组成的二维数组，可以表示线性变换。
4. **高阶张量**：
   - 三维或更高维度的数组。例如，3阶张量可以看作是矩阵的数组。

# 反向传播算法

















































































 
