# 神经网络

Neural Network，又叫人工神经网络。

用于对函数进行估计和近似的数学模型。

# MP神经元模型

$$
t = f(W^{T}A+b)
$$

# 单层神经网络

单层神经网络，通常指的是由输入层和一个神经元层组成的最简单形式的神经网络。在这种网络中，输入直接连接到输出，中间没有隐藏层。这种单层结构通常用于实现简单的线性或非线性映射。

1. **单层神经网络的结构**：
   - **输入层**：由输入特征组成，这些特征是网络要处理的数据。
   - **输出层（神经元层）**：由一个或多个神经元组成。每个神经元对输入数据进行加权求和，然后通过一个激活函数生成输出。
2. **单层神经元的输出为向量的原因**：
   - 当我们谈论单层神经网络的“单层”时，这通常指的是单个的输出层。如果输出层包含多个神经元，每个神经元都会对相同的输入进行操作，但根据它们各自的权重和偏置产生不同的输出值。
   - 因此，如果输出层有多个神经元，网络的输出将是一个向量，其中每个元素对应一个神经元的输出。例如，如果输出层有3个神经元，那么网络的输出将是一个包含3个数值的向量，每个数值是一个神经元的输出。
3. **应用**：
   - 单层神经网络通常用于简单的任务，例如二元分类（单个输出神经元）或多类分类（输出层中每个类别一个神经元）。
   - 由于它们的结构简单，单层神经网络不能捕捉复杂的数据模式，这限制了它们的应用范围。对于更复杂的任务，通常需要使用多层神经网络（即深度学习模型）。

总的来说，单层神经网络的输出是向量的原因在于输出层可以包含多个神经元，每个神经元对输入数据进行独立的处理和输出。这种结构使得网络能够同时进行多个简单的运算，如多个二元分类或单个多类分类任务。

# 感知机

感知机类似于神经网络中的神经元（即神经网络的每个节点）。

# 多层神经网络

包含输入层，输出层和隐藏层。

每一层神经元个数不确定。

# 全连接层

在神经网络中，一个“全连接层”指的是一个网络层，其中该层的每个神经元都与前一层中的每个神经元相连。这意味着在这种层中，输入层的每个节点（神经元）都全面连接到输出层的每个节点。

全连接层在前一层的输出的基础上进行了一次$Y=Wx+b$的变化。

特点：

1. **密集连接**：每个输入节点（或神经元）都与下一层的每个节点相连。这意味着每个输入对下一层中每个神经元的输出都有影响。
2. **参数数量**：由于每个连接都有一个对应的权重，所以全连接层通常包含大量的参数。例如，如果前一层有 n 个神经元，当前层有 m 个神经元，那么全连接层就会有 $n×m$ 个权重，加上 m 个偏置。
3. **功能**：全连接层主要用于综合前面层提取的特征，并基于这些特征做出最终的预测或分类。

# 激活函数

激活函数在神经网络中起着至关重要的作用。它们是应用于神经元输出的函数，用于引入非线性特性到网络中。没有激活函数，神经网络将无法学习和模拟复杂的数据如图像、视频、音频和类似的数据。激活函数帮助神经网络解决非线性问题，使其能够进行复杂模式的学习和预测。

激活函数的关键特点：

1. **非线性**：激活函数通常是非线性的。这使得神经网络能够捕捉到输入数据中的复杂和非线性关系。
2. **帮助学习**：通过非线性转换，激活函数帮助神经网络学习从输入到输出的复杂映射。没有它们，网络无论有多少层，其实际上只能表达线性变换。
3. **决定神经元是否激活**：激活函数决定了在给定的输入下一个神经元是否应该被“激活”或输出信号。

常见的激活函数：

1. **Sigmoid或Logistic函数**：这是一个将输入压缩到0和1之间的S型曲线函数。它在早期神经网络中非常流行，尤其是用于二分类问题。
2. **ReLU（Rectified Linear Unit）**：ReLU函数提供了一个简单的非线性变换。如果输入为正，则输出该输入；如果输入为负，则输出0。这个函数在现代神经网络中非常流行，因为它比Sigmoid或Tanh函数计算上更有效，并且在训练深度网络时效果更好。
3. **Tanh（双曲正切函数）**：Tanh函数将输入压缩到-1和1之间。它与Sigmoid相似，但其输出范围是对称的。
4. **Softmax函数**：Softmax函数常用于多类分类的输出层。它将输入转换为一组表示概率分布的输出



为什么需要激活函数？

如果没有激活函数，神经网络的每一层本质上只能执行线性变换。线性方程无论如何组合，其结果仍然是线性的，这限制了网络的表达能力。非线性激活函数使得神经网络能够学习和表示更加复杂的函数，从而可以处理更复杂的数据，如图像、声音、语言等。

# Tensor

 张量，简单理解多维数据。

比如：

1. **Scaler标量（0阶张量）**：
   - 一个普通的数字，无方向，例如温度或质量。
2. **Vector向量（1阶张量）**：
   - 由数字组成的一维数组，有方向，例如速度或加速度。
3. **Matrix矩阵（2阶张量）**：
   - 数字组成的二维数组，可以表示线性变换。
4. **高阶张量**：
   - 三维或更高维度的数组。例如，3阶张量可以看作是矩阵的数组。

# 反向传播算法

















































































 
